---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Name: Viren Velacheri, EID: vv6898

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information

I am a big sports lover and one of my favorite sports is basketball. I played competitively through middle school and was a part of the school team and still play it today as a side hobby. In basketball, there are various metrics/stats used to evaluate players and teams.

The dataset I chose contains some common variables such as the players and number of games played, but instead of some of the basic statistics that are commonly brought up, this dataset contains more advanced metrics such as win shares, box plus/minus, etc. I found this dataset through the usual google search and the reason this specfic dataset stood out to me, as implied above, is because of the different metrics of data they contain. I am a basketball lover and look forward to exploring these dataset! I expect there to be some obvious associations between some of the metrics, but my goal is to more look at by position and see potentially if these advanced metrics tend to favor certain positions over others.


```{R}
library(tidyverse)
library(stringr)
# read your datasets in here, e.g., with read_csv()

# if your dataset needs tidying, do so here
dataset <- read_csv("project2_dataset.csv") 
# removed some of the unnecessary columns 
dataset <- dataset %>% select(-'X20') %>% select(-'X25')
# function that I use in creating binary variable for determining whether player
# is a guard or not (1 means player, 0 means it is not)
guardOrNot <- function(posVector) {
  guardBinary <- vector()
  guardCheck <- c("SG", "PG")
  for(pos in posVector) {
    if(pos %in% guardCheck) {
      guardBinary <- c(guardBinary, 1)
    } else {
      guardBinary <- c(guardBinary, 0)
    }
  }
  return(guardBinary)
}
# add guard or not column to dataset
dataset <- dataset %>% mutate(guardOrNot = guardOrNot(Pos))
glimpse(dataset)
# removed Rk column since it is irrelevant to what I am doing
dataset <- dataset %>% select(-"Rk")
# removed any rows with NA values
dataset <- dataset %>% na.omit
```

### Cluster Analysis

```{R} 
# your code here
library(cluster)
# clustering code here
cluster_selection <- dataset %>% select(PER, WS, VORP)
sil_width1 <- vector()
for (i in 2:10) {
    kms <- kmeans(cluster_selection, centers = i)  #compute k-means solution for each k
    sil <- silhouette(kms$cluster, dist(dataset$`TS%`))  #get sil widths
    sil_width1[i] <- mean(sil[, 3])  #take averages (higher is better)
}
ggplot() + geom_line(aes(x = 1:10, y = sil_width1)) + 
    scale_x_continuous(name = "k", breaks = 1:10)


ts_pam <- cluster_selection %>% pam(k = 2)
ts_pam$silinfo$avg.width

library(GGally)

cluster_selection <- cluster_selection %>% ungroup %>% mutate(cluster = as.factor(ts_pam$clustering))

ggpairs(cluster_selection, aes(color = cluster))

```

Discussion of clustering here
Based on the first graph, the k that gives the best silhouette width is the value 2.The average silhouette width though is only 0.4551869, so the structure present is weak and could be artificial. Since this produced this k value produced the largest average silhouette width though, I picked 2 clusters. The reason I chose the 3 numeric variables that I did was because they seemed the most intriguing of the lot and I didn't want to pick more as then the plots would be less interpretable. 

Based on the clusters, the red cluster has higher WS (win shares) and VORP than the blue cluster. This appears to suggest that the red cluster represents players with higher win shares and VORP, while the blue cluster represents players with lower win shares and VORP. There is some overlap between the clusters, but not too much. The clusters have some overlap on the PER variable. There is also some very strong correlation between the variables VORP and WS (0.883). There is also some moderate correlation between the variables WS and PER (0.575) and VORP and PER (0.560). Overall, there is some good correlation between these respective variables.
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
cluster_selection <- dataset %>% select(PER, WS, VORP)
pca1 <- princomp(cluster_selection, cor=T)
summary(pca1)
scaledability <- data.frame(scale(cluster_selection))
summary(pca1, loadings = "T")
corr_matrix <- pca1$scores %>% cor %>% round(10)

matrix_scores <- pca1$scores %>% as.data.frame 
matrix_scores %>% ggplot(aes(pca1$scores[, 1], pca1$scores[, 
    2])) + geom_point() + xlab("PCA1") + ylab("PCA2") + ggtitle("Principal Component Analysis")

library(factoextra)
fviz_pca_biplot(pca1)


```

Discussions of PCA here. 
I keep the first 2 PCS as they account for over 96% of the variance. PC1 accounts for the highest variability amongst the three variables at 0.78565. It has positive values for all three of the variables, so this means if someone scores high on PC1, they score high on all three of the variables (PER, WS, VORP) and vice versa if someone scores low on PC1, they score low on all three of the variables. PC2 has not nearly as high a variability at 0.175 and has only a positive value for the variable PER. This means if someone scores high on PC2, they score high only on PER and low on the other two variables (WS and VORP). Likewise, if someone scores low on PC2, they score low on PER and high on the other two variables (WS and VORP).  

###  Linear Classifier

```{R}
# linear classifier code here
linear_fit <- lm(guardOrNot ~ PER + WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = dataset)
prob_reg <- predict(linear_fit)
class_diag(prob_reg, dataset$guardOrNot, positive = 1)


```

```{R}
# cross-validation of linear classifier here
set.seed(322)
k = 10

data <- sample_frac(dataset)  #randomly order rows
folds <- rep(1:k, length.out = nrow(data))  #create folds

diags <- NULL

i = 1
for (i in 1:k) {
    # create training and test sets
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$guardOrNot
    
    # train model
    fit <- lm(guardOrNot ~ PER + WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = train, 
    family = "binomial")
    
    # test model
    probs <- predict(fit, test)  ### GET PREDICTIONS FROM THE TRAINED MODEL ON THE TEST SET HERE
    
    # get performance metrics for each fold
    diags <- rbind(diags, class_diag(probs, truth, positive = 1))
}

# average performance metrics across all folds
summarize_all(diags, mean)
```

Discussion here
The AUC for the linear regression is 0.7119, while for the 10-fold CV it is 0.68567. This appears to imply that the linear regression model is doing a slightly better job of predicting new observations and that the 10-fold CV might be possibly showing slight signs of overfitting since the AUC is just a tad bit smaller.


### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(guardOrNot ~ PER + WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = dataset)
prob_knn <- predict(knn_fit, dataset)
class_diag(prob_knn[, 2], dataset$guardOrNot, positive = 1)
```

```{R}
# cross-validation of np classifier here
set.seed(322)
k = 10

data <- sample_frac(dataset)  #randomly order rows
folds <- rep(1:k, length.out = nrow(data))  #create folds

diags <- NULL

i = 1
for (i in 1:k) {
    # create training and test sets
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$guardOrNot
    
    # train model
    fit <- knn3(guardOrNot ~ PER + WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = train)  ### SPECIFY THE LOGISTIC REGRESSION MODEL FIT TO THE TRAINING SET HERE
    
    # test model
    probs <- predict(fit, test)  ### GET PREDICTIONS FROM THE TRAINED MODEL ON THE TEST SET HERE
    
    # get performance metrics for each fold
    diags <- rbind(diags, class_diag(probs[, 2], truth, positive = 1))
}

# average performance metrics across all folds
summarize_all(diags, mean)
```

Discussion
The AUC for KNN was 0.7706, while for the 10-fold CV it had a significantly lower AUC at a value of 0.51547. This means that KNN not only did a better job, but also that there are some major signs of overfitting since the AUC for the 10-fold CV is way smalller.

### Regression/Numeric Prediction

```{R}
# regression model code here
fit <- lm(PER ~ WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = dataset)

yhat <- predict(fit)
cbind(yhat, y = dataset$PER)

mean((dataset$PER - yhat)^2)

```

```{R}
# cross-validation of regression model here
set.seed(322)
k = 5

data <- sample_frac(dataset)  #randomly order rows
folds <- rep(1:k, length.out = nrow(data))  #create folds

diags <- NULL

i = 1
for (i in 1:k) {
    # create training and test sets
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$guardOrNot
    
    # train model
    fit <- lm(PER ~ WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = train)  ### SPECIFY THE LOGISTIC REGRESSION MODEL FIT TO THE TRAINING SET HERE
    
    # test model
    yhat <- predict(fit, newdata = test)
    
    # get performance metrics for each fold
    diags <- mean((dataset$PER - yhat)^2)
}

# average performance metrics across all folds
mean(diags)

```

Discussion
For linear regression the MSE value of 4.152874 was calculated which is very good and implies that the prediction error is not too bad. The 5-fold CV had a bigger MSE at a value of 69.85123 which is definitely bigger, but still not too bad. The prediction error is definitely bigger, but still alright.

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3", required = F)
ball<-"Ball is"
cat(c(ball,py$life))
```

```{python}
# python code here
life = 'Life!'
print(r.ball, life)
```

Discussion
First, in R, ball is defined as "Ball is". From there, variable life in python is defined as "Life!". They are each defined in different environments, but still can be used in each others respective environment thanks to the reticulate package. In R environment, the python variable life is accessed with the use of the py$ syntax and with the use of the cat command, the "Ball is Life!" line is printed. Meanwhile, in the python environment, the ball variable from r is accessed using the r. syntax and so with the use of the python print statement, the same saying is printed out. This shows how we are able to grab variables from the different environments and use them accordingly.  

### Concluding Remarks

I had a great semester and really learned a lot from this class. I look forward to utilizing all these skills in the future! Thank you!




