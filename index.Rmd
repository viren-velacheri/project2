---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Name: Viren Velacheri, EID: vv6898

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information

I am a big sports lover and one of my favorite sports is basketball. I played competitively through middle school and was a part of the school team and still play it today as a side hobby. In basketball, there are various metrics/stats used to evaluate players and teams.

The dataset I chose contains some common variables such as the players and number of games played, but instead of some of the basic statistics that are commonly brought up, this dataset contains more advanced metrics such as win shares, box plus/minus, etc. I found this dataset through the usual google search and the reason this specfic dataset stood out to me, as implied above, is because of the different metrics of data they contain. I am a basketball lover and look forward to exploring these dataset! I expect there to be some obvious associations between some of the metrics, but my goal is to more look at by position and see potentially if these advanced metrics tend to favor certain positions over others.


```{R}
library(tidyverse)
library(stringr)
# read your datasets in here, e.g., with read_csv()

# if your dataset needs tidying, do so here
dataset <- read_csv("project2_dataset.csv") 
# removed some of the unnecessary columns 
dataset <- dataset %>% select(-'X20') %>% select(-'X25')
# function that I use in creating binary variable for determining whether player
# is a guard or not (1 means player, 0 means it is not)
guardOrNot <- function(posVector) {
  guardBinary <- vector()
  guardCheck <- c("SG", "PG")
  for(pos in posVector) {
    if(pos %in% guardCheck) {
      guardBinary <- c(guardBinary, 1)
    } else {
      guardBinary <- c(guardBinary, 0)
    }
  }
  return(guardBinary)
}
# add guard or not column to dataset
dataset <- dataset %>% mutate(guardOrNot = guardOrNot(Pos))
glimpse(dataset)
# removed Rk column since it is irrelevant to what I am doing
dataset <- dataset %>% select(-"Rk")
# removed any rows with NA values
dataset <- dataset %>% na.omit
glimpse(dataset)
```

### Cluster Analysis

```{R} 
# your code here
library(cluster)
# clustering code here
cluster_selection <- dataset %>% select(PER, `TS%`, OWS, DWS, OBPM, DBPM, VORP)
sil_width1 <- vector()
for (i in 2:10) {
    kms <- kmeans(cluster_selection, centers = i)  #compute k-means solution for each k
    sil <- silhouette(kms$cluster, dist(dataset$`TS%`))  #get sil widths
    sil_width1[i] <- mean(sil[, 3])  #take averages (higher is better)
}
ggplot() + geom_line(aes(x = 1:10, y = sil_width1)) + 
    scale_x_continuous(name = "k", breaks = 1:10)


ts_pam <- cluster_selection %>% pam(k = 2)
ts_pam$silinfo$avg.width

library(GGally)

cluster_selection %>% mutate(cluster = as.factor(ts_pam$clustering)) %>% 
    ggpairs(columns = 1:7, aes(color = cluster))

```

Discussion of clustering here
Based on the first graph, the k that gives the best silhouette width is the value 2.The average silhouette width though is only 0.40154, so the structure present is weak and could be artificial. Since this produced this k value produced the largest average silhouette width though, I picked 2 clusters. The reason I chose the 7 numeric variables that I did was because they seemed the most intriguing of the lot and I didn't want to pick more as then the plots would be less interpretable. 

As far as clusters go and the overall goodness of fit of the solution, it doesn't seem too bad. There is some solid overlap for some of the variables such as true shooting percentage, PER, OBPM, and DBPM. For the other variables like offensive win shares, defensive win shares, and VORP though the goodness of fit of the cluster solution isn't too good. I also noticed there is some solid correlation between some of the variables such as OBPM and PER (0.925), True shooting percentage and PER (0.766), and VORP and OWS (0.882).
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
pca1 <- princomp(cluster_selection, cor=T)
summary(pca1)
corr_matrix <- pca1$scores %>% cor %>% round(10)

matrix_scores <- pca1$scores %>% as.data.frame 
matrix_scores %>% ggplot(aes(pca1$scores[, 1], pca1$scores[, 
    2])) + geom_point() + xlab("PCA1") + ylab("PCA2") + ggtitle("Principal Component Analysis")

library(factoextra)
fviz_pca_biplot(pca1)
# eigval <- temp_dataset_pca$sdev^2
# varprop <- round(eigval/sum(eigval), 2)
# ggplot() + geom_bar(aes(y=varprop, x=1:7), stat="identity") + xlab("")
# geom_text(aes(x=1:7, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) +
# scale_x_continuous(breaks=1:10)
# 
# temp_df <- data.frame(PC1=temp_dataset_pca$scores[, 1],PC2=temp_dataset_pca$scores[, 2])
# ggplot(temp_df, aes(PC1, PC2)) + geom_point()
# ggplot() + geom_bar(aes(y=varprop, x=1:7), stat='identity') + xlab("") + 


```

Discussions of PCA here. 
It appears that non guards are more extreme in terms of the scores on PC1 and PC2, while guards are less extreme and seem more in the middle. 

###  Linear Classifier

```{R}
# linear classifier code here
logistic_fit <- lm(guardOrNot ~ PER + WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = dataset)
prob_reg <- predict(logistic_fit)
class_diag(prob_reg, dataset$guardOrNot, positive = 1)


```

```{R}
# cross-validation of linear classifier here
set.seed(322)
k = 10

data <- sample_frac(dataset)  #randomly order rows
folds <- rep(1:k, length.out = nrow(data))  #create folds

diags <- NULL

i = 1
for (i in 1:k) {
    # create training and test sets
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$guardOrNot
    
    # train model
    fit <- lm(guardOrNot ~ PER + WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = train, 
    family = "binomial")
    
    # test model
    probs <- predict(fit, test)  ### GET PREDICTIONS FROM THE TRAINED MODEL ON THE TEST SET HERE
    
    # get performance metrics for each fold
    diags <- rbind(diags, class_diag(probs, truth, positive = 1))
}

# average performance metrics across all folds
summarize_all(diags, mean)
```

Discussion here

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(guardOrNot ~ PER + WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = dataset)
prob_knn <- predict(knn_fit, dataset)
class_diag(prob_knn[, 2], dataset$guardOrNot, positive = 1)
```

```{R}
# cross-validation of np classifier here
set.seed(322)
k = 10

data <- sample_frac(dataset)  #randomly order rows
folds <- rep(1:k, length.out = nrow(data))  #create folds

diags <- NULL

i = 1
for (i in 1:k) {
    # create training and test sets
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$guardOrNot
    
    # train model
    fit <- knn3(guardOrNot ~ PER + WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = train)  ### SPECIFY THE LOGISTIC REGRESSION MODEL FIT TO THE TRAINING SET HERE
    
    # test model
    probs <- predict(fit, test)  ### GET PREDICTIONS FROM THE TRAINED MODEL ON THE TEST SET HERE
    
    # get performance metrics for each fold
    diags <- rbind(diags, class_diag(probs[, 2], truth, positive = 1))
}

# average performance metrics across all folds
summarize_all(diags, mean)
```

Discussion


### Regression/Numeric Prediction

```{R}
# regression model code here
fit <- lm(PER ~ WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = dataset)

yhat <- predict(fit)
cbind(yhat, y = dataset$PER)

mean((dataset$PER - yhat)^2)

```

```{R}
# cross-validation of regression model here
set.seed(322)
k = 5

data <- sample_frac(dataset)  #randomly order rows
folds <- rep(1:k, length.out = nrow(data))  #create folds

diags <- NULL

i = 1
for (i in 1:k) {
    # create training and test sets
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$guardOrNot
    
    # train model
    fit <- lm(PER ~ WS + BPM + VORP + FTr + OWS + DWS + OBPM + DBPM + MP, data = train)  ### SPECIFY THE LOGISTIC REGRESSION MODEL FIT TO THE TRAINING SET HERE
    
    # test model
    yhat <- predict(fit, newdata = test)
    
    # get performance metrics for each fold
    diags <- mean((dataset$PER - yhat)^2)
}

# average performance metrics across all folds
mean(diags)

```

Discussion

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3", required = F)
hi<-"Hello"
cat(c(hi,py$hi))
```

```{python}
# python code here
hi = 'world'
print(r.hi, hi)
```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




